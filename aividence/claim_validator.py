"""
Core claim validator for AIvidence.

Author: Zaoqu Liu
Email: liuzaoqu@163.com
GitHub: https://github.com/Zaoqu-Liu
"""

import os
import json
import time
import re
from datetime import datetime
from typing import List, Dict, Any, Optional, Union
import concurrent.futures

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from Bio import Entrez
from tqdm.auto import tqdm
import faiss
from sentence_transformers import SentenceTransformer
from openai import OpenAI


class ClaimValidator:
    """
    Framework for validating scientific claims generated by LLMs against real scientific literature.
    """
    
    def __init__(
        self, 
        email: str,
        llm_api_key: str,
        embedding_model: str = "pubmedbert",
        embedding_device: str = "cpu",
        llm_model: str = "claude-3-7-sonnet-20250219",
        llm_base_url: str = "https://chat.cloudapi.vip/v1/",
        cache_dir: str = "validation_cache",
        max_workers: int = 4
    ):
        """
        Initialize the LLM claim validator.
        
        Args:
            email: Email for PubMed API
            llm_api_key: API key for LLM service
            embedding_model: Name or path to embedding model
            embedding_device: Device to use for embeddings (cpu or cuda)
            llm_model: Model to use for evaluation
            llm_base_url: Base URL for LLM service
            cache_dir: Directory to cache results
            max_workers: Number of workers for parallel processing
        """
        self.email = email
        Entrez.email = email
        self.max_workers = max_workers
        
        # Setup embedding model
        print(f"Loading embedding model: {embedding_model}...")
        if embedding_model == "pubmedbert":
            model_path = "NeuML/pubmedbert-base-embeddings"
        else:
            model_path = embedding_model
        self.embedding_model = SentenceTransformer(model_path, device=embedding_device)
        
        # Setup LLM client
        self.llm_client = OpenAI(
            api_key=llm_api_key,
            base_url=llm_base_url,
            default_headers={"x-foo": "true"}
        )
        self.llm_model = llm_model
        
        # Create cache directory if it doesn't exist
        self.cache_dir = cache_dir
        if not os.path.exists(cache_dir):
            os.makedirs(cache_dir)
            
        # Initialize tracking variables
        self.claim = None
        self.abstracts = []
        self.metadata = []
        self.evaluations = []
        self.distances = []
        self.indices = []
        self.summary = None
            
    def validate_claim(
        self, 
        claim: str,
        search_terms: Optional[List[str]] = None,
        max_papers_to_analyze: int = 30,
        show_plot: bool = True
    ) -> Dict:
        """
        Validate a scientific claim against the literature.
        
        Args:
            claim: Scientific claim to validate
            search_terms: Optional list of search terms. If None, will generate from claim
            max_papers_to_analyze: Maximum number of papers to analyze in detail
            show_plot: Whether to display plots directly
            
        Returns:
            Dictionary with validation results
        """
        self.claim = claim
        print(f"Validating claim: {claim}")
        
        # Generate search terms if not provided
        if search_terms is None:
            search_terms = self._generate_search_terms(claim)
            
        # Fetch literature
        self._fetch_literature(search_terms)
        
        # If no abstracts found, return early
        if not self.abstracts:
            return {
                "claim": claim,
                "validity": "unknown",
                "confidence": 0.0,
                "message": "No relevant literature found to validate claim",
                "timestamp": datetime.now().isoformat()
            }
        
        # Encode abstracts and find most relevant
        self._find_relevant_papers(claim, max_papers=max_papers_to_analyze)
        
        # Evaluate relevant papers
        self._evaluate_papers()
        
        # Synthesize results
        self._synthesize_findings()
        
        # Display results if requested
        if show_plot:
            self.visualize_results()
            
        return self.summary
    
    def _generate_search_terms(self, claim: str) -> List[str]:
        """
        Generate comprehensive search terms from a claim using LLM.
        
        Args:
            claim: Scientific claim
            
        Returns:
            List of search terms
        """
        # Use the LLM to generate comprehensive search terms
        prompt = f"""Analyze this scientific claim and generate comprehensive PubMed search terms to validate it.
Don't limit the number of terms - include as many relevant search terms as possible.

CLAIM: {claim}

Guidelines for generating search terms:
1. Focus on the key relationships in the claim (not just isolated keywords)
2. Include the main entity + action combinations (e.g., "EntityX inhibits")
3. Include the main entity + target process (e.g., "EntityX migration")
4. Include any alternative names for biological entities (e.g., "VAP-1" for "AOC3")
5. Include the entity by itself to get general information about it
6. Create different combinations that would capture all relevant literature
7. Be comprehensive - we want to find ALL papers that could validate this claim

Return ONLY the search terms as a simple list, one per line, without numbering or explanation.
"""
        
        try:
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2
            )
            
            # Parse the response to extract search terms
            search_terms = []
            lines = response.choices[0].message.content.strip().split('\n')
            
            for line in lines:
                term = line.strip()
                if term and not term.startswith('#') and not term.startswith('*'):
                    # Remove any leading dashes, numbers or bullets
                    term = re.sub(r'^[-â€¢*\d.)\s]+', '', term).strip()
                    if term:
                        search_terms.append(term)
            
            # Ensure we have at least some basic search terms if the LLM fails
            if not search_terms:
                raise Exception("No valid search terms generated")
                
        except Exception as e:
            print(f"Error generating search terms with LLM: {e}")
            # Fallback: extract basic terms
            search_terms = []
            
            # Extract capitalized entities (potential genes/proteins)
            capitalized_terms = re.findall(r'\b[A-Z][A-Za-z0-9]*\b', claim)
            
            if capitalized_terms:
                main_entity = capitalized_terms[0]
                search_terms.append(main_entity)
                
                # Add basic context
                if "cancer" in claim.lower():
                    search_terms.append(f"{main_entity} cancer")
                if "tumor" in claim.lower():
                    search_terms.append(f"{main_entity} tumor")
                if "migration" in claim.lower():
                    search_terms.append(f"{main_entity} migration")
                if "progression" in claim.lower():
                    search_terms.append(f"{main_entity} progression")
                    
                # Add known synonyms for common entities
                if main_entity == "AOC3":
                    search_terms.append("VAP-1")
                    search_terms.append("Amine oxidase copper containing 3")
            else:
                # No identifiable entity, use key phrases
                words = claim.split()
                key_terms = [w for w in words if len(w) > 3 and w.lower() not in 
                            ["with", "that", "this", "from", "then", "than", "when"]]
                search_terms = key_terms[:5]
        
        # Deduplicate while preserving order
        seen = set()
        unique_terms = []
        for term in search_terms:
            if term.lower() not in seen:
                seen.add(term.lower())
                unique_terms.append(term)
        
        print(f"Generated search terms: {unique_terms}")
        return unique_terms
        
    def _fetch_literature(self, search_terms: List[str]) -> None:
        """
        Fetch scientific literature based on search terms using parallel processing.
        
        Args:
            search_terms: List of search terms
        """
        all_abstracts = []
        all_metadata = []
        
        # Create executor for parallel processing
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit tasks for each search term
            future_to_term = {executor.submit(self._fetch_term, term): term for term in search_terms}
            
            # Process results as they complete with a progress bar
            for future in tqdm(concurrent.futures.as_completed(future_to_term), 
                              total=len(future_to_term), 
                              desc="Fetching search terms"):
                term = future_to_term[future]
                try:
                    term_abstracts, term_metadata = future.result()
                    all_abstracts.extend(term_abstracts)
                    all_metadata.extend(term_metadata)
                except Exception as e:
                    print(f"Error fetching results for term '{term}': {e}")
        
        # Remove duplicates by PMID
        unique_abstracts = []
        unique_metadata = []
        seen_pmids = set()
        
        for abstract, metadata in zip(all_abstracts, all_metadata):
            pmid = metadata.get('pmid')
            if pmid and pmid not in seen_pmids:
                seen_pmids.add(pmid)
                unique_abstracts.append(abstract)
                unique_metadata.append(metadata)
                
        print(f"Found {len(unique_abstracts)} unique papers after deduplication")
        
        # Store for later use
        self.abstracts = unique_abstracts
        self.metadata = unique_metadata
    
    def _fetch_term(self, term: str) -> tuple:
        """
        Fetch papers for a single search term.
        
        Args:
            term: Search term
            
        Returns:
            Tuple of (abstracts, metadata)
        """
        cache_file = os.path.join(self.cache_dir, f"search_{hash(term) % 10000}.json")
        
        # Check cache first
        if os.path.exists(cache_file):
            with open(cache_file, 'r') as f:
                cache_data = json.load(f)
                
            print(f"Loaded {len(cache_data['abstracts'])} results from cache for '{term}'")
            return cache_data['abstracts'], cache_data['metadata']
                
        print(f"Searching for: {term}")
        
        # Maximum results per query - large enough to get comprehensive coverage
        max_results = 200
        
        # Get paper IDs from PubMed
        try:
            handle = Entrez.esearch(db="pubmed", term=term, retmax=max_results)
            record = Entrez.read(handle)
            id_list = record["IdList"]
            
            if not id_list:
                print(f"No results found for '{term}'")
                return [], []
                
            print(f"Found {len(id_list)} results for '{term}'")
            
            # Fetch in batches to avoid timeouts
            batch_size = 50
            term_abstracts = []
            term_metadata = []
            
            for i in range(0, len(id_list), batch_size):
                batch_ids = id_list[i:i+batch_size]
                
                try:
                    handle = Entrez.efetch(db="pubmed", id=batch_ids, rettype="medline", retmode="text")
                    records = handle.read()
                    
                    # Process records
                    record_list = records.split("\n\n\n")
                    for record in record_list:
                        if not record.strip():
                            continue
                            
                        # Parse the record
                        paper_data = self._parse_pubmed_record(record)
                        
                        if paper_data and paper_data.get('abstract'):
                            term_abstracts.append(paper_data['abstract'])
                            
                            # Create metadata entry
                            metadata_entry = {k: v for k, v in paper_data.items() if k != 'abstract'}
                            term_metadata.append(metadata_entry)
                            
                except Exception as e:
                    print(f"Error fetching batch for '{term}': {e}")
            
            # Cache the results
            cache_data = {
                'abstracts': term_abstracts,
                'metadata': term_metadata
            }
            
            with open(cache_file, 'w') as f:
                json.dump(cache_data, f)
            
            return term_abstracts, term_metadata
                
        except Exception as e:
            print(f"Error searching for '{term}': {e}")
            return [], []
        
    def _parse_pubmed_record(self, record: str) -> Dict:
        """
        Parse a PubMed record in MEDLINE format.
        
        Args:
            record: MEDLINE record text
            
        Returns:
            Dictionary with extracted data
        """
        result = {
            'pmid': None,
            'title': None,
            'abstract': None,
            'authors': [],
            'year': None,
            'journal': None,
            'doi': None
        }
        
        if not record.strip():
            return None
            
        lines = record.split("\n")
        
        # Extract data fields
        for i, line in enumerate(lines):
            if line.startswith("PMID- "):
                result['pmid'] = line[6:].strip()
            elif line.startswith("TI  - "):
                # Title might span multiple lines
                result['title'] = line[6:].strip()
                j = i + 1
                while j < len(lines) and lines[j].startswith("      "):
                    result['title'] += " " + lines[j].strip()
                    j += 1
            elif line.startswith("AB  - "):
                # Abstract might span multiple lines
                result['abstract'] = line[6:].strip()
                j = i + 1
                while j < len(lines) and lines[j].startswith("      "):
                    result['abstract'] += " " + lines[j].strip()
                    j += 1
            elif line.startswith("AU  - "):
                result['authors'].append(line[6:].strip())
            elif line.startswith("DP  - "):
                # Extract year from date
                date_parts = line[6:].strip().split()
                if date_parts:
                    try:
                        result['year'] = int(date_parts[0][:4])
                    except:
                        pass
            elif line.startswith("JT  - "):
                result['journal'] = line[6:].strip()
            elif line.startswith("LID  - "):
                # Try to extract DOI
                lid = line[6:].strip()
                if "[doi]" in lid:
                    result['doi'] = lid.split("[doi]")[0].strip()
        
        return result
        
    def _find_relevant_papers(self, claim: str, max_papers: int = 30) -> None:
        """
        Find papers most relevant to the claim using semantic search.
        
        Args:
            claim: Scientific claim
            max_papers: Maximum number of papers to return
        """
        if not self.abstracts:
            print("No abstracts available for analysis")
            return
            
        print(f"Encoding {len(self.abstracts)} abstracts...")
        
        # Encode abstracts
        corpus_embeddings = self.embedding_model.encode(
            self.abstracts, 
            show_progress_bar=True,
            convert_to_numpy=True
        )
        
        # Encode claim
        claim_embedding = self.embedding_model.encode([claim], convert_to_numpy=True)
        
        # Build FAISS index
        dimension = corpus_embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)
        index.add(corpus_embeddings.astype('float32'))
        
        # Find most similar abstracts
        k = min(max_papers, len(self.abstracts))
        D, I = index.search(claim_embedding.astype('float32'), k=k)
        
        # Store for later use
        self.distances = D[0]
        self.indices = I[0]
        
        # Print selected abstracts
        print(f"\nSelected {k} most relevant papers for detailed evaluation:")
        for i, idx in enumerate(self.indices):
            metadata = self.metadata[idx]
            year = metadata.get("year", "Unknown")
            journal = metadata.get("journal", "Unknown journal")
            print(f"{i+1}. [{year}] {metadata['title'][:80]}... ({journal})")
            
    def _evaluate_papers(self) -> None:
        """
        Evaluate the stance of each relevant paper towards the claim.
        """
        if not hasattr(self, 'indices') or len(self.indices) == 0:
            print("No relevant papers found for evaluation")
            return
            
        print("\nEvaluating papers against claim...")
        evaluations = []
        
        # Use a progress bar for evaluation
        for i, idx in enumerate(tqdm(self.indices, desc="Evaluating papers")):
            abstract = self.abstracts[idx]
            metadata = self.metadata[idx]
            
            prompt = f"""You are evaluating whether a scientific paper supports or refutes a claim.

CLAIM: {self.claim}

PAPER ABSTRACT: {abstract}

Assess whether this abstract supports, refutes, or is neutral toward the claim. Consider:
1. Directness of evidence (does it directly address the claim?)
2. Research methodology (is the evidence strong?)
3. Consistency with the claim's mechanism
4. Conflicts of interest or limitations

First, provide your stance as exactly one of these words: SUPPORT, REFUTE, or NEUTRAL.
Then on a new line, provide your confidence level as a number between 0 and 1.
Then on a new line, provide your reasoning.
Then on a new line, provide the evidence quality as: STRONG, MODERATE, or WEAK.
Then on a new line, provide the relevance as: HIGH, MEDIUM, or LOW.
"""

            try:
                response = self.llm_client.chat.completions.create(
                    model=self.llm_model,
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.2
                )
                
                # Parse the text response
                response_text = response.choices[0].message.content.strip().split('\n')
                
                # Extract parts
                stance = "neutral"  # Default
                confidence = 0.5    # Default
                reasoning = ""
                evidence_quality = "moderate"
                relevance = "medium"
                
                # Parse first line for stance
                if len(response_text) > 0:
                    first_line = response_text[0].upper()
                    if "SUPPORT" in first_line:
                        stance = "support"
                    elif "REFUTE" in first_line:
                        stance = "refute"
                    else:
                        stance = "neutral"
                
                # Try to extract confidence from second line
                if len(response_text) > 1:
                    try:
                        # Find numbers in second line
                        confidence_matches = re.findall(r"0\.\d+|\d+\.\d+|\d+", response_text[1])
                        if confidence_matches:
                            confidence = float(confidence_matches[0])
                            # Ensure in 0-1 range
                            confidence = max(0, min(confidence, 1))
                    except:
                        pass
                
                # Extract reasoning from remaining lines
                if len(response_text) > 2:
                    reasoning = " ".join(response_text[2:])
                    
                    # Try to extract evidence quality
                    if "STRONG" in reasoning.upper():
                        evidence_quality = "strong"
                    elif "WEAK" in reasoning.upper():
                        evidence_quality = "weak"
                    else:
                        evidence_quality = "moderate"
                        
                    # Try to extract relevance
                    if "HIGH" in reasoning.upper():
                        relevance = "high"
                    elif "LOW" in reasoning.upper():
                        relevance = "low"
                    else:
                        relevance = "medium"
                
                result = {
                    "stance": stance,
                    "confidence": confidence,
                    "reasoning": reasoning,
                    "evidence_quality": evidence_quality,
                    "relevance": relevance,
                    "consistency": 5  # Default medium consistency
                }
                
                evaluations.append(result)
                
            except Exception as e:
                print(f"Error in evaluation: {str(e)[:100]}...")
                
                # Use a simplified prompt as fallback
                try:
                    simple_prompt = f"""Evaluate if this abstract supports or refutes the claim:
Claim: {self.claim}
Abstract: {abstract}
Answer with ONLY one word: support, refute, or neutral."""

                    simple_response = self.llm_client.chat.completions.create(
                        model=self.llm_model,
                        messages=[{"role": "user", "content": simple_prompt}],
                        temperature=0.1
                    )
                    
                    stance = simple_response.choices[0].message.content.lower().strip()
                    if stance in ["support", "refute", "neutral"]:
                        evaluations.append({
                            "stance": stance,
                            "confidence": 0.6,
                            "reasoning": "Simple evaluation due to API limitations",
                            "evidence_quality": "moderate",
                            "relevance": "medium",
                            "consistency": 5
                        })
                    else:
                        # Fallback
                        evaluations.append({
                            "stance": "neutral",
                            "confidence": 0.5,
                            "reasoning": "Unable to evaluate accurately",
                            "evidence_quality": "weak",
                            "relevance": "low",
                            "consistency": 3
                        })
                except:
                    # Last resort fallback
                    evaluations.append({
                        "stance": "neutral",
                        "confidence": 0.5,
                        "reasoning": "Unable to evaluate",
                        "evidence_quality": "weak",
                        "relevance": "low",
                        "consistency": 3
                    })
                    
            # Add small delay to avoid rate limiting
            time.sleep(0.5)
            
        self.evaluations = evaluations
        
        # Print summary of evaluations
        stances = [e.get("stance", "neutral") for e in evaluations]
        stance_counts = {
            "support": stances.count("support"),
            "refute": stances.count("refute"),
            "neutral": stances.count("neutral")
        }
        print(f"Evaluation summary: {stance_counts}")
        
    def _synthesize_findings(self) -> None:
        """
        Synthesize the evaluation results into an overall assessment.
        """
        if not self.evaluations:
            print("No evaluations available for synthesis")
            self.summary = {
                "claim": self.claim,
                "validity": "unknown",
                "confidence": 0.0,
                "message": "No evaluations available",
                "timestamp": datetime.now().isoformat()
            }
            return
            
        # Count stances
        stances = [e.get("stance", "neutral") for e in self.evaluations]
        stance_counts = {
            "support": stances.count("support"),
            "refute": stances.count("refute"),
            "neutral": stances.count("neutral")
        }
        
        # Calculate weighted scores based on confidence and evidence quality
        support_score = 0
        refute_score = 0
        total_weight = 0
        
        for eval_result in self.evaluations:
            if "stance" not in eval_result:
                continue
                
            # Calculate weight based on evidence quality and relevance
            quality_weight = {
                "strong": 1.0,
                "moderate": 0.7,
                "weak": 0.3
            }.get(eval_result.get("evidence_quality", "moderate"), 0.5)
            
            relevance_weight = {
                "high": 1.0,
                "medium": 0.7,
                "low": 0.3
            }.get(eval_result.get("relevance", "medium"), 0.5)
            
            confidence = eval_result.get("confidence", 0.5)
            
            # Combined weight
            weight = quality_weight * relevance_weight * confidence
            total_weight += weight
            
            # Add to appropriate score
            if eval_result["stance"] == "support":
                support_score += weight
            elif eval_result["stance"] == "refute":
                refute_score += weight
        
        # Normalize scores
        if total_weight > 0:
            support_score /= total_weight
            refute_score /= total_weight
            
        # Calculate overall validity score (-1 to 1 scale)
        validity_score = support_score - refute_score
        
        # Convert to validity assessment
        if validity_score > 0.3:
            validity = "supported"
            confidence = min(validity_score + 0.5, 1.0)  # Scale to 0.5-1.0 range
        elif validity_score < -0.3:
            validity = "refuted"
            confidence = min(abs(validity_score) + 0.5, 1.0)  # Scale to 0.5-1.0 range
        else:
            validity = "inconclusive"
            confidence = 0.5 - abs(validity_score * 0.5)  # Higher for more neutral
            
        print(f"Synthesis complete. Verdict: {validity} (Confidence: {confidence:.2f})")
            
        # Get detailed synthesis from LLM
        synthesis_prompt = f"""As a scientific evaluator, assess the validity of this claim based on literature evidence:

CLAIM: {self.claim}

EVIDENCE SUMMARY:
- Supporting studies: {stance_counts['support']}
- Refuting studies: {stance_counts['refute']}
- Neutral studies: {stance_counts['neutral']}
- Weighted support score: {support_score:.2f}
- Weighted refute score: {refute_score:.2f}

Based on this evidence, provide:
1. A verdict on whether the claim is supported, refuted, or inconclusive
2. An explanation of your reasoning
3. Discussion of the strength and limitations of evidence
4. Assessment of whether this claim produced by an LLM appears scientifically valid

Keep your response concise but comprehensive.
"""

        try:
            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=[{"role": "user", "content": synthesis_prompt}],
                temperature=0.3
            )
            
            llm_synthesis = response.choices[0].message.content
        except Exception as e:
            print(f"Error generating synthesis: {e}")
            llm_synthesis = "Unable to generate synthesis."
            
        # Create overall summary
        self.summary = {
            "claim": self.claim,
            "validity": validity,
            "confidence": confidence,
            "support_score": support_score,
            "refute_score": refute_score,
            "stance_counts": stance_counts,
            "total_papers_analyzed": len(self.evaluations),
            "total_papers_retrieved": len(self.abstracts),
            "llm_synthesis": llm_synthesis,
            "timestamp": datetime.now().isoformat()
        }
        
    def generate_report(self, output_file: Optional[str] = None) -> str:
        """
        Generate a comprehensive report of the validation results.
        
        Args:
            output_file: Optional file to save the report to
            
        Returns:
            Report text
        """
        if not hasattr(self, 'summary') or not self.summary:
            return "No validation results available."
            
        print("Generating validation report...")
            
        # Create header
        report = f"""# Scientific Claim Validation Report

## Claim Analyzed
"{self.claim}"

## Validation Verdict
- **Status**: {self.summary['validity'].capitalize()}
- **Confidence**: {self.summary['confidence']:.2f}/1.00

## Evidence Summary
- **Total papers retrieved**: {self.summary['total_papers_retrieved']}
- **Papers analyzed in detail**: {self.summary['total_papers_analyzed']}
- **Supporting evidence**: {self.summary['stance_counts']['support']} papers
- **Refuting evidence**: {self.summary['stance_counts']['refute']} papers
- **Neutral evidence**: {self.summary['stance_counts']['neutral']} papers

## Scientific Assessment
{self.summary['llm_synthesis']}

## Evidence Details
"""
        
        # Add key papers
        supporting_papers = []
        refuting_papers = []
        
        for i, (eval_result, idx) in enumerate(zip(self.evaluations, self.indices)):
            if "stance" not in eval_result:
                continue
                
            metadata = self.metadata[idx]
            authors = metadata.get("authors", ["Unknown"])
            first_author = authors[0] if authors else "Unknown"
            year = metadata.get("year", "Unknown")
            title = metadata.get("title", "Untitled")
            journal = metadata.get("journal", "Unknown journal")
            pmid = metadata.get("pmid", "")
            
            paper_entry = {
                "citation": f"{first_author} et al. ({year}). {title}. {journal}.",
                "pmid": pmid,
                "year": year,
                "stance": eval_result["stance"],
                "confidence": eval_result.get("confidence", 0),
                "evidence_quality": eval_result.get("evidence_quality", "Unknown"),
                "relevance": eval_result.get("relevance", "Unknown"),
                "reasoning": eval_result.get("reasoning", "")
            }
            
            if eval_result["stance"] == "support" and eval_result.get("confidence", 0) > 0.6:
                supporting_papers.append(paper_entry)
            elif eval_result["stance"] == "refute" and eval_result.get("confidence", 0) > 0.6:
                refuting_papers.append(paper_entry)
                
        # Sort by confidence * quality
        supporting_papers.sort(key=lambda x: x["confidence"], reverse=True)
        refuting_papers.sort(key=lambda x: x["confidence"], reverse=True)
        
        # Add supporting papers
        if supporting_papers:
            report += "\n### Key Supporting Papers\n"
            for i, paper in enumerate(supporting_papers[:5]):  # Top 5
                report += f"{i+1}. {paper['citation']}\n"
                report += f"   - PMID: {paper['pmid']}\n"
                report += f"   - Confidence: {paper['confidence']:.2f}, Quality: {paper['evidence_quality']}\n"
                report += f"   - Rationale: {paper['reasoning']}\n\n"
        else:
            report += "\n### No Strong Supporting Papers Found\n"
            
        # Add refuting papers
        if refuting_papers:
            report += "\n### Key Refuting Papers\n"
            for i, paper in enumerate(refuting_papers[:5]):  # Top 5
                report += f"{i+1}. {paper['citation']}\n"
                report += f"   - PMID: {paper['pmid']}\n"
                report += f"   - Confidence: {paper['confidence']:.2f}, Quality: {paper['evidence_quality']}\n"
                report += f"   - Rationale: {paper['reasoning']}\n\n"
        else:
            report += "\n### No Strong Refuting Papers Found\n"
            
        # Add methodology
        report += f"""
## Methodology
This assessment was conducted using:
1. Comprehensive PubMed literature retrieval
2. Semantic embedding using science-specific language models
3. Evidence assessment of the most relevant studies
4. Weighted synthesis based on evidence quality and relevance

## Timestamp
Report generated on {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
"""
        
        # Save if requested
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"Report saved to: {output_file}")
                
        return report
        
    def visualize_results(self) -> plt.Figure:
        """
        Create visualization of validation results and display directly.
        
        Returns:
            Matplotlib figure
        """
        if not hasattr(self, 'evaluations') or not self.evaluations:
            fig, ax = plt.subplots()
            ax.text(0.5, 0.5, "No validation results available.", 
                   ha='center', va='center')
            plt.show()
            return fig
            
        print("Generating visualization...")
            
        # Create figure with multiple subplots
        fig, axs = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(f"Validation Results: {self.claim[:70]}...", fontsize=16)
        
        # 1. Stance distribution
        stances = [e.get("stance", "unknown") for e in self.evaluations]
        stance_counts = {
            "support": stances.count("support"),
            "refute": stances.count("refute"),
            "neutral": stances.count("neutral"),
            "unknown": stances.count("unknown")
        }
        
        # Remove zeros
        stance_counts = {k: v for k, v in stance_counts.items() if v > 0}
        
        colors = {
            "support": "green", 
            "refute": "red", 
            "neutral": "gray",
            "unknown": "lightgray"
        }
        
        axs[0, 0].bar(
            list(stance_counts.keys()), 
            list(stance_counts.values()),
            color=[colors[s] for s in stance_counts.keys()]
        )
        axs[0, 0].set_title('Evidence Stance Distribution')
        axs[0, 0].set_ylabel('Number of Papers')
        
        # 2. Evidence quality
        qualities = [e.get("evidence_quality", "unknown") for e in self.evaluations]
        quality_counts = {
            "strong": qualities.count("strong"),
            "moderate": qualities.count("moderate"),
            "weak": qualities.count("weak"),
            "unknown": qualities.count("unknown")
        }
        
        # Remove zeros
        quality_counts = {k: v for k, v in quality_counts.items() if v > 0}
        
        quality_colors = {
            "strong": "darkgreen", 
            "moderate": "orange", 
            "weak": "tomato", 
            "unknown": "lightgray"
        }
        
        axs[0, 1].bar(
            list(quality_counts.keys()),
            list(quality_counts.values()),
            color=[quality_colors[q] for q in quality_counts.keys()]
        )
        axs[0, 1].set_title('Evidence Quality Distribution')
        axs[0, 1].set_ylabel('Number of Papers')
        
        # 3. Publication years
        if hasattr(self, 'indices') and len(self.indices) > 0:
            years = [self.metadata[idx].get("year") for idx in self.indices]
            years = [y for y in years if y is not None]
            
            if years:
                year_counts = {}
                for year in years:
                    year_counts[year] = year_counts.get(year, 0) + 1
                    
                # Sort by year
                sorted_years = sorted(year_counts.keys())
                counts = [year_counts[y] for y in sorted_years]
                
                axs[1, 0].bar(sorted_years, counts, color='steelblue')
                axs[1, 0].set_title('Publication Year Distribution')
                axs[1, 0].set_xlabel('Year')
                axs[1, 0].set_ylabel('Number of Papers')
                axs[1, 0].tick_params(axis='x', rotation=45)
            else:
                axs[1, 0].text(0.5, 0.5, "No year data available", 
                              ha='center', va='center')
        else:
            axs[1, 0].text(0.5, 0.5, "No papers analyzed", 
                          ha='center', va='center')
            
        # 4. Weighted verdict visualization
        if hasattr(self, 'summary') and self.summary:
            support_score = self.summary.get("support_score", 0)
            refute_score = self.summary.get("refute_score", 0)
            
            verdict_data = {
                "Evidence\nSupporting": support_score,
                "Evidence\nRefuting": refute_score
            }
            
            verdict_colors = ['green', 'red']
            
            axs[1, 1].bar(
                list(verdict_data.keys()),
                list(verdict_data.values()),
                color=verdict_colors
            )
            axs[1, 1].set_title('Weighted Evidence Scores')
            axs[1, 1].set_ylabel('Weighted Score')
            axs[1, 1].set_ylim(0, 1)
            
            # Add verdict as text
            validity = self.summary.get("validity", "inconclusive").capitalize()
            confidence = self.summary.get("confidence", 0)
            
            verdict_text = f"Verdict: {validity}\nConfidence: {confidence:.2f}"
            axs[1, 1].text(0.5, 0.9, verdict_text, 
                          ha='center', va='top',
                          transform=axs[1, 1].transAxes,
                          bbox=dict(facecolor='white', alpha=0.8))
        else:
            axs[1, 1].text(0.5, 0.5, "No verdict data available", 
                          ha='center', va='center')
        
        plt.tight_layout(rect=[0, 0, 1, 0.95])
        plt.show()  # Display directly
            
        return fig